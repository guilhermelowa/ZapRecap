{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "FILE_NAME = 'cvRonny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_new_message(line):\n",
    "    # Check if line starts with date pattern DD/MM/YYYY HH:MM\n",
    "    date_pattern = r'^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}'\n",
    "    return bool(re.match(date_pattern, line))\n",
    "\n",
    "def parse_line(line):\n",
    "    # Split line into date and message\n",
    "    date, message = line.split(' - ', 1)\n",
    "    date = datetime.strptime(date, '%d/%m/%Y %H:%M')\n",
    "    return date, message\n",
    "\n",
    "def parse_message(message):\n",
    "    # Split message into author and content\n",
    "    try:\n",
    "        author, content = message.split(': ', 1)\n",
    "        return author, content\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "def store_message(messages, date, author, content):\n",
    "    # Store message in messages dictionary\n",
    "    try:\n",
    "        messages[author].append((date, content))\n",
    "    except KeyError:\n",
    "        messages[author] = [(date, content)]\n",
    "    except TypeError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "author_and_messages = {}\n",
    "current_author = None\n",
    "current_message = None\n",
    "\n",
    "with open(f'C:\\\\Users\\\\guiga\\\\Documents\\\\ZapRecap\\\\test\\\\{FILE_NAME}.txt', 'r', encoding='utf-8') as file:\n",
    "    # Skip first line (default WhatsApp messages)\n",
    "    next(file)\n",
    "\n",
    "    date, message = parse_line(next(file))\n",
    "    dates.append(date)\n",
    "    current_author, current_message = parse_message(message)\n",
    "\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if is_new_message(line):\n",
    "            store_message(author_and_messages, date, current_author, current_message)\n",
    "            date, message = parse_line(line)\n",
    "            dates.append(date)\n",
    "            current_author, current_message = parse_message(message)\n",
    "        else:\n",
    "            current_message += ' ' + line\n",
    "            \n",
    "    # Store last message\n",
    "    store_message(author_and_messages, date, current_author, current_message)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from dates list\n",
    "df = pd.DataFrame({'datetime': dates})\n",
    "\n",
    "# Add necessary columns for heatmap\n",
    "df['weekday'] = df['datetime'].dt.dayofweek\n",
    "df['month_year'] = df['datetime'].dt.strftime('%m/%Y')\n",
    "df['week'] = df['datetime'].dt.isocalendar().week\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "pivot = pd.pivot_table(\n",
    "    df,\n",
    "    values='datetime',\n",
    "    index='weekday',\n",
    "    columns=['month_year', 'week'],\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Flatten multi-index columns\n",
    "pivot.columns = [f\"{m}_{w}\" for m, w in pivot.columns]\n",
    "\n",
    "# Normalize data using percentile-based scaling\n",
    "vmax = np.percentile(pivot.values[pivot.values > 0], 95)  # 95th percentile\n",
    "vmin = np.percentile(pivot.values[pivot.values > 0], 5)   # 5th percentile\n",
    "\n",
    "# Create heatmap with green colormap\n",
    "plt.figure(figsize=(20, 5))\n",
    "ax = plt.gca()\n",
    "plt.imshow(pivot, cmap='Greens', aspect='auto', vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='Message Count')\n",
    "\n",
    "# Add labels\n",
    "plt.yticks(range(7), calendar.day_name)\n",
    "\n",
    "# Create x-axis labels (show month only at transitions)\n",
    "month_labels = []\n",
    "prev_month = None\n",
    "for col in pivot.columns:\n",
    "    month = col.split('_')[0]\n",
    "    if month != prev_month:\n",
    "        month_labels.append(month)\n",
    "        prev_month = month\n",
    "    else:\n",
    "        month_labels.append('')\n",
    "\n",
    "plt.xticks(range(len(pivot.columns)), month_labels, rotation=45)\n",
    "plt.xlabel('Month/Year')\n",
    "plt.ylabel('Day of Week')\n",
    "\n",
    "# Remove grid and border\n",
    "ax.grid(False)\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize variables\n",
    "conversation_messages = []\n",
    "current_conversation = []\n",
    "max_conversation = []\n",
    "time_threshold = 30 * 60  # 30 minutes in seconds\n",
    "\n",
    "# Group messages into conversations\n",
    "for i in range(len(dates)-1):\n",
    "    current_conversation.append(dates[i])\n",
    "    \n",
    "    # Calculate time difference with next message\n",
    "    time_diff = (dates[i+1] - dates[i]).total_seconds()\n",
    "    \n",
    "    # If gap is more than 30 minutes, end current conversation\n",
    "    if time_diff > time_threshold:\n",
    "        if len(current_conversation) > len(max_conversation):\n",
    "            max_conversation = current_conversation.copy()\n",
    "        conversation_messages.append(len(current_conversation))\n",
    "        current_conversation = []\n",
    "\n",
    "# Handle last message\n",
    "current_conversation.append(dates[-1])\n",
    "conversation_messages.append(len(current_conversation))\n",
    "if len(current_conversation) > len(max_conversation):\n",
    "    max_conversation = current_conversation\n",
    "\n",
    "# Calculate average\n",
    "avg_length = sum(conversation_messages) / len(conversation_messages)\n",
    "\n",
    "print(f\"Average conversation length: {avg_length:.2f} messages\")\n",
    "print(f\"Longest conversation: {len(max_conversation)} messages\")\n",
    "print(f\"Longest conversation period: {max_conversation[0]} to {max_conversation[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_per_author = {}\n",
    "message_lengths = {}\n",
    "\n",
    "# Calculate both message counts and average lengths in a single loop\n",
    "for author, messages in author_and_messages.items():\n",
    "    if author is not None:\n",
    "        messages_per_author[author] = len(messages)\n",
    "        total_length = sum(len(msg[1]) for msg in messages)\n",
    "        message_lengths[author] = total_length / len(messages)\n",
    "\n",
    "# Sort results by message count\n",
    "sorted_messages = dict(sorted(messages_per_author.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print results\n",
    "for author in sorted_messages:\n",
    "    print(f\"{author}:\")\n",
    "    print(f\"  Messages: {sorted_messages[author]}\")\n",
    "    print(f\"  Avg length: {message_lengths[author]:.2f} characters per message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get Portuguese stop words\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Add custom stop words\n",
    "custom_stop_words = {'pra', 'tá', 'q', 'tb', 'né', 'tô', 'ta', 'to', 'mídia', 'oculta'}\n",
    "stop_words.update(custom_stop_words)\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def process_text(text):\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and short words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Process all messages\n",
    "all_words = []\n",
    "for author, messages in author_and_messages.items():\n",
    "    if author is not None:  # Skip None author\n",
    "        for _, content in messages:\n",
    "            words = process_text(content)\n",
    "            all_words.extend(words)\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Get the 20 most common words\n",
    "most_common = word_counts.most_common(20)\n",
    "\n",
    "# Print results\n",
    "print(\"20 Most Common Words:\")\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palavrões mais usados\n",
    "Quem xinga mais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavroes = {'porra', 'caralho', 'merda', 'foda', 'fodase', 'foda-se', 'puta', 'putas', 'putinha', 'putinhas', 'putao'}\n",
    "\n",
    "# Process messages and count palavroes per author\n",
    "palavroes_count = {}\n",
    "palavroes_by_author = defaultdict(Counter)\n",
    "palavroes_frequency = Counter()\n",
    "\n",
    "for author, messages in author_and_messages.items():\n",
    "    if author is not None:\n",
    "        palavroes_count[author] = 0\n",
    "        for _, content in messages:\n",
    "            content = content.lower()\n",
    "            for palavra in palavroes:\n",
    "                count = content.count(palavra)\n",
    "                if count > 0:\n",
    "                    palavroes_count[author] += count\n",
    "                    palavroes_by_author[author][palavra] += count\n",
    "                    palavroes_frequency[palavra] += count\n",
    "\n",
    "# Print palavroes per author\n",
    "print(\"Palavrões por autor:\")\n",
    "for author, count in sorted(palavroes_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{author}: {count} palavrões\")\n",
    "    print(\"  Mais usados:\", \", \".join(f\"{p}({c})\" for p, c in palavroes_by_author[author].most_common(3)))\n",
    "\n",
    "print(\"\\nPalavrões mais comuns no total:\")\n",
    "for palavra, count in palavroes_frequency.most_common():\n",
    "    print(f\"{palavra}: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WhatsApp Analyzer",
   "language": "python",
   "name": "whatsapp-analyzer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
